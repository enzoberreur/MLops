"""Streamlit UI allowing API or local MinIO inference for the classifier."""
from __future__ import annotations

import io
import os
import tempfile
from contextlib import suppress
from pathlib import Path
from typing import Any, Dict, List

import requests
import streamlit as st
import torch
from minio.error import S3Error
from PIL import Image

from models.model import DandelionClassifier
from models.utils import CLASS_NAMES, get_inference_transform, get_minio_client

API_URL = os.environ.get("API_URL", "http://localhost:8000")
MINIO_MODEL_BUCKET = os.environ.get("MINIO_MODEL_BUCKET", "dandelion-models")
MINIO_MODEL_PREFIX = os.environ.get("MINIO_MODEL_PREFIX", "models/")
IMAGE_SIZE = int(os.environ.get("IMAGE_SIZE", 128))
DEVICE = torch.device("cpu")
TRANSFORM = get_inference_transform(IMAGE_SIZE)

st.set_page_config(page_title="Dandelion vs Grass", page_icon="üåº")
st.title("üåº Dandelion or Grass?")
st.write(
    "Testez vos mod√®les PyTorch en direct depuis MinIO ou interrogez l'API "
    "d√©ploy√©e pour obtenir des pr√©dictions."
)


@st.cache_data(show_spinner=False)
def list_available_models() -> List[str]:
    """Retourne la liste des checkpoints stock√©s sur MinIO."""
    client = get_minio_client()
    try:
        objects = client.list_objects(MINIO_MODEL_BUCKET, prefix=MINIO_MODEL_PREFIX, recursive=True)
        return sorted(
            obj.object_name
            for obj in objects
            if obj.object_name.lower().endswith(".pt")
        )
    except S3Error as exc:  # pragma: no cover - d√©pend du r√©seau
        st.warning(f"Impossible de lister les mod√®les dans MinIO : {exc}")
        return []


@st.cache_resource(show_spinner=False)
def load_model_from_minio(object_name: str) -> DandelionClassifier:
    """T√©l√©charge un checkpoint depuis MinIO et le charge en m√©moire."""
    client = get_minio_client()
    with tempfile.NamedTemporaryFile(suffix=".pt", delete=False) as tmp_file:
        local_path = Path(tmp_file.name)
    try:
        client.fget_object(MINIO_MODEL_BUCKET, object_name, str(local_path))
        checkpoint = torch.load(local_path, map_location=DEVICE)
    finally:
        with suppress(FileNotFoundError):
            local_path.unlink(missing_ok=True)

    class_names = checkpoint.get("class_names", CLASS_NAMES)
    model = DandelionClassifier(num_classes=len(class_names))
    model.load_state_dict(checkpoint["model_state_dict"])
    model.class_names = class_names  # type: ignore[attr-defined]
    model.eval()
    model.to(DEVICE)
    return model


def render_api_mode(uploaded_file: Any) -> None:
    """Envoie l'image √† l'API FastAPI pour obtenir une pr√©diction."""
    if uploaded_file and st.button("Pr√©dire via l'API"):
        st.info("Envoi de l'image √† l'API‚Ä¶")
        try:
            response = requests.post(
                f"{API_URL}/predict",
                files={
                    "file": (
                        uploaded_file.name,
                        io.BytesIO(uploaded_file.getvalue()),
                        uploaded_file.type or "image/jpeg",
                    )
                },
                timeout=30,
            )
            response.raise_for_status()
            payload: Dict[str, Any] = response.json()
        except requests.RequestException as exc:
            st.error(f"Appel API impossible : {exc}")
            return
        st.success(f"Pr√©diction : {payload['prediction']} (confiance {payload['confidence']:.2f})")
        st.json(payload.get("class_probabilities", {}))
    else:
        st.caption("Configurez l'URL de l'API dans la barre lat√©rale si n√©cessaire.")


def render_local_mode(uploaded_file: Any) -> None:
    """Effectue l'inf√©rence localement en t√©l√©chargeant un mod√®le depuis MinIO."""
    model_choices = list_available_models()
    if not model_choices:
        st.warning("Aucun mod√®le .pt trouv√© dans le bucket MinIO.")
        return

    selected_model = st.selectbox("Choisissez un mod√®le", model_choices)

    if uploaded_file and st.button("Pr√©dire avec le mod√®le local"):
        with st.spinner("T√©l√©chargement du mod√®le‚Ä¶"):
            try:
                model = load_model_from_minio(selected_model)
            except S3Error as exc:
                st.error(f"Impossible de t√©l√©charger le mod√®le : {exc}")
                return

        image = Image.open(uploaded_file).convert("RGB")
        tensor = TRANSFORM(image).unsqueeze(0).to(DEVICE)
        with torch.no_grad():
            outputs = model(tensor)
            probabilities = torch.softmax(outputs, dim=1)[0]
            confidence, predicted_idx = torch.max(probabilities, dim=0)

        class_names = getattr(model, "class_names", CLASS_NAMES)
        prediction = class_names[predicted_idx.item()]
        st.success(f"Pr√©diction : {prediction} (confiance {confidence.item():.2f})")
        st.json(
            {
                class_names[i]: round(float(probabilities[i].item()), 4)
                for i in range(len(class_names))
            }
        )


with st.sidebar:
    st.header("Configuration")
    api_url = st.text_input("URL de l'API FastAPI", value=API_URL)
    if api_url != API_URL:
        API_URL = api_url
    mode = st.radio("Mode d'inf√©rence", ["API distante", "Local (MinIO)"])
    st.caption("Le mode local t√©l√©charge directement un checkpoint et ex√©cute l'inf√©rence ici.")

uploaded_file = st.file_uploader("Choisissez une image", type=["jpg", "jpeg", "png"])
if uploaded_file:
    st.image(uploaded_file, caption="Image t√©l√©charg√©e", use_column_width=True)
else:
    st.info("Ajoutez une image pour lancer une pr√©diction.")

if uploaded_file:
    if mode == "API distante":
        render_api_mode(uploaded_file)
    else:
        render_local_mode(uploaded_file)

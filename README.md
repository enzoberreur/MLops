# MLOps Dandelion Classifier

Projet MLOps minimaliste pour classifier des images de pissenlits et d'herbe.

## Stack

- **Orchestration** : Apache Airflow 2
- **Mod√©lisation** : PyTorch + torchvision
- **MLOps** : MLflow (tracking), MinIO (artifacts & datasets)
- **Serving** : FastAPI + Streamlit
- **Conteneurisation** : Docker, Docker Compose, Kubernetes (Minikube)
- **CI/CD** : GitHub Actions
- **Monitoring** : Prometheus + Grafana (optionnel)

## Choix techniques & objectifs

- **Airflow + MinIO** : pipeline d√©claratif pour automatiser le t√©l√©chargement des images, la pr√©paration et l'entra√Ænement. MinIO offre un stockage S3 compatible reproductible en local et en cluster.
- **PyTorch** : mod√®le l√©ger (ResNet18) finement ajust√© pour un dataset restreint (200 images par classe) avec des transformations simples (resize + augmentation l√©g√®re) afin de limiter l'overfitting.
- **MLflow Tracking** : centralise les m√©triques (loss, accuracy, f1) et l'enregistrement du mod√®le. Chaque run conserve le code source et permet la comparaison de plusieurs entra√Ænements.
- **FastAPI** : service de pr√©diction synchrone qui t√©l√©charge le dernier checkpoint depuis MinIO au d√©marrage, expose `/predict` et `/health`, et publie des m√©triques Prometheus.
- **Streamlit** : interface pour tester rapidement les pr√©dictions et monitorer la disponibilit√© de l'API (mode local ou API distante).
- **Prometheus + Grafana** : supervision des appels API (`/predict`) et de la sant√© du scheduler Airflow via `statsd-exporter`. Deux dashboards sont provisionn√©s automatiquement.
- **CI/CD GitHub Actions** : pipeline unique (tests ‚Üí build ‚Üí d√©ploiement Minikube) pour valider la stack bout-en-bout. Les images sont publi√©es sur GHCR et peuvent √™tre retag√©es pour Docker Hub.

### R√©sultats observ√©s (r√©f√©rence)

Les m√©triques d√©pendent fortement des tirages al√©atoires (split train/val). Avec la configuration par d√©faut (`IMAGE_SIZE=224`, `EPOCHS=5`), un run typique donne :

- **Accuracy validation** : 0.90 ¬± 0.03
- **F1-score validation** : 0.89 ¬± 0.04
- **Temps d'entra√Ænement** : ~4 minutes sur CPU (LocalExecutor Airflow)

### Captures d'√©cran principales

| Vue | Description |
| --- | ----------- |
| ![Run MLflow](docs/screenshots/mlflow.png) | Vue d'ensemble de l'exp√©rience `dandelion-classifier`, incluant les param√®tres et le succ√®s du run. |
| ![Courbes MLflow](docs/screenshots/mlflow_metrics.png) | Historique des m√©triques (loss & accuracy) captur√©es pendant l'entra√Ænement PyTorch. |
| ![Stockage MinIO](docs/screenshots/minio.png) | Buckets `dandelion-models` et `mlflow-artifacts` contenant les jeux de donn√©es et le checkpoint `models/latest/best_model.pt`. |
| ![API Streamlit](docs/screenshots/streamlit.png) | Interface utilisateur pour tester les pr√©dictions depuis un navigateur. |
| ![Dashboard API Grafana](docs/screenshots/grafana_calssifier.png) | Monitoring temps r√©el du trafic `/predict` (latence p90 et nombre d'appels). |
| ![Dashboard Airflow Grafana](docs/screenshots/airflow_data_pipeline.png) | Supervision du scheduler Airflow et du throughput des t√¢ches via StatsD exporter. |

> Captures r√©alis√©es sur l‚Äôenvironnement Docker Compose (identique √† l‚Äôenvironnement Minikube c√¥t√© services).

## Images Docker

- **GHCR (par d√©faut CI/CD)** : `https://ghcr.io/enzoberreur/mlops/mlops-app:<commit-sha>`
- **Publication Docker Hub (option)** :
  ```bash
  export DOCKERHUB_USER=enzoberreur
  docker pull ghcr.io/enzoberreur/mlops/mlops-app:<commit-sha>
  docker tag ghcr.io/enzoberreur/mlops/mlops-app:<commit-sha> docker.io/$DOCKERHUB_USER/mlops-app:<commit-sha>
  docker push docker.io/$DOCKERHUB_USER/mlops-app:<commit-sha>
  ```
  Cr√©ez le repository Docker Hub `docker.io/enzoberreur/mlops-app` puis communiquez les URLs des tags pertinents (ex. `https://hub.docker.com/r/enzoberreur/mlops-app/tags?name=<commit-sha>`).

## Art√©facts de production

- **Buckets MinIO** :
  - Donn√©es brutes : `dandelion-data/raw/`
  - Donn√©es pr√©trait√©es : `dandelion-data/processed/`
  - Mod√®le : `dandelion-models/models/latest/best_model.pt`
  - Artifacts MLflow : `mlflow-artifacts/<experiment-id>/<run-id>/artifacts/`
- **MLflow Experiment** : `dandelion-classifier`
  - Les runs sont tagg√©s avec l'ID Airflow (`dag_id`, `execution_date`) pour tracer leur provenance.
  - Reproductibilit√© garantie par la sauvegarde du mod√®le `mlflow.pytorch`.
- **Airflow Vars/Connections** : g√©r√©es via `.env` et inject√©es dans les containers (`MINIO_*`, `MLFLOW_TRACKING_URI`, etc.). Export possible via `airflow variables export`.

## Architecture

```mermaid
graph TB
    subgraph DataSource["üì¶ Data Source"]
        GH[GitHub Raw Images]
    end
    
    subgraph Orchestration["üîÑ Orchestration Layer"]
        AF[Airflow Scheduler]
        AFW[Airflow Webserver :8080]
        DAG1[dandelion_data_pipeline]
        DAG2[dandelion_retrain_pipeline]
    end
    
    subgraph Storage["üíæ Storage Layer"]
        MINIO[MinIO S3 :9000/:9001]
        PG[(PostgreSQL)]
        BUCKET1[dandelion-data]
        BUCKET2[dandelion-models]
        BUCKET3[mlflow-artifacts]
    end
    
    subgraph MLOps["ü§ñ ML Training & Tracking"]
        MLF[MLflow Tracking :5500]
        TRAIN[PyTorch ResNet18]
    end
    
    subgraph Serving["üöÄ Serving Layer"]
        API[FastAPI :8000]
        STR[Streamlit :8501]
    end
    
    subgraph Monitoring["üìä Monitoring Layer"]
        PROM[Prometheus :9090]
        GRAF[Grafana :3000]
        STATSD[StatsD Exporter :9102]
    end
    
    subgraph CICD["‚öôÔ∏è CI/CD"]
        GHA[GitHub Actions]
        K8S[Minikube/K8s]
    end
    
    GH -->|download images| DAG1
    DAG1 -->|create & download| AF
    DAG1 -->|upload raw| BUCKET1
    DAG1 -->|preprocess| AF
    DAG1 -->|upload processed| BUCKET1
    DAG1 -->|train model| TRAIN
    
    DAG2 -->|sync data| BUCKET1
    DAG2 -->|retrain| TRAIN
    
    TRAIN -->|log metrics| MLF
    TRAIN -->|save artifacts| BUCKET3
    TRAIN -->|save model| BUCKET2
    MLF -->|store| MINIO
    
    API -->|load model| BUCKET2
    STR -->|upload image| API
    STR -->|or load from| MINIO
    API -->|predict| STR
    
    AF --> PG
    AFW --> PG
    MINIO --> BUCKET1
    MINIO --> BUCKET2
    MINIO --> BUCKET3
    
    API -->|expose /metrics| PROM
    AF -->|statsd| STATSD
    STATSD -->|metrics| PROM
    PROM -->|scrape| GRAF
    
    GHA -->|build & push| GHA
    GHA -->|deploy| K8S
    K8S -->|apply| API
    K8S -->|apply| STR
    
    classDef storage fill:#e1f5ff,stroke:#01579b,stroke-width:2px
    classDef compute fill:#fff3e0,stroke:#e65100,stroke-width:2px
    classDef serving fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
    classDef monitoring fill:#e8f5e9,stroke:#1b5e20,stroke-width:2px
    classDef orchestration fill:#fff9c4,stroke:#f57f17,stroke-width:2px
    
    class MINIO,PG,BUCKET1,BUCKET2,BUCKET3 storage
    class TRAIN,MLF,GHA,K8S compute
    class API,STR serving
    class PROM,GRAF,STATSD monitoring
    class AF,AFW,DAG1,DAG2 orchestration
```

## Pr√©requis

- Docker & Docker Compose
- Python 3.10+ (pour ex√©cuter les scripts en local)
- Minikube (pour l'environnement "prod")

## D√©marrage rapide local (Docker Compose)

1. Copier le fichier d'exemple d'environnement et ajuster les variables :
   ```bash
   cp .env.example .env
   python -c "import secrets; print(secrets.token_urlsafe(32))"  # g√©n√©rer AIRFLOW_FERNET_KEY
   ```
2. Construire les images :
   ```bash
   docker compose build
   ```
3. Initialiser Airflow :
   ```bash
   docker compose up airflow-init
   ```
4. Lancer l'ensemble des services :
   ```bash
   docker compose up
   ```

Services expos√©s :
- Airflow UI : http://localhost:8080 (login/password `admin/admin`)
- FastAPI : http://localhost:8000/docs
- Streamlit : http://localhost:8501
- MLflow : http://localhost:5000
- MinIO : http://localhost:9001 (console)
- Prometheus : http://localhost:9090
- Grafana : http://localhost:3000 (admin/admin)
- Tableau de bord Grafana "Dandelion Classifier" (m√©triques API) et "Airflow Overview" (Scheduler + DagRun)

Sur Streamlit, vous pouvez choisir entre deux modes d'inf√©rence :
- **API distante** : envoie l'image √† FastAPI (`/predict`).
- **Local (MinIO)** : t√©l√©charge un checkpoint `.pt` depuis MinIO et ex√©cute l'inf√©rence directement dans l'application.

> Astuce : ex√©cuter une premi√®re fois `python scripts/bootstrap_minio.py` pour cr√©er les buckets sur MinIO (si vous n'utilisez pas Docker Compose).

### Pipelines Airflow

- `dandelion_data_pipeline` :
  1. T√©l√©charge les images des deux classes depuis GitHub.
  2. Stocke les donn√©es brutes et pr√©-trait√©es dans MinIO.
  3. Lance l'entra√Ænement du mod√®le, loggue dans MLflow et pousse le mod√®le sur MinIO.
- `dandelion_retrain_pipeline` : synchronise les derni√®res donn√©es pr√©trait√©es depuis MinIO puis relance l'entra√Ænement.

### Monitoring

Prometheus scrappe l'endpoint `/metrics` expos√© par FastAPI. Le dashboard Grafana (import√© automatiquement) montre :
- compteur total des pr√©dictions,
- latence p90 de l'endpoint `/predict`.

L'exporter StatsD collecte √©galement les m√©triques Airflow (scheduler heartbeat, DagRun success/fail) accessibles dans le dashboard "Airflow Overview".

## D√©veloppement local hors Docker

```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
python scripts/bootstrap_minio.py
python models/train.py --data-dir data/processed  # supposant des donn√©es d√©j√† t√©l√©charg√©es
uvicorn app.api.main:app --reload
streamlit run app/webapp/streamlit_app.py
```

## Tests

```bash
pytest
```

## D√©ploiement Minikube

1. D√©marrer minikube :
   ```bash
   minikube start --driver=docker
   ```
2. Construire et charger l'image :
   ```bash
   docker build -t mlops-app:latest .
   minikube image load mlops-app:latest
   ```
3. D√©ployer :
   ```bash
   kubectl apply -f k8s/minikube-manifest.yaml
   ```
4. Consulter les services :
   ```bash
   kubectl get svc -n mlops
   minikube service --namespace mlops api
   minikube service --namespace mlops streamlit
   ```

## CI/CD GitHub Actions

- `tests` : installe les d√©pendances et ex√©cute `pytest`.
- `build` : construit l'image Docker et la pousse sur GHCR (`ghcr.io/<repo>/mlops-app`).
- `deploy` : d√©marre Minikube dans le runner, charge l'image publi√©e et applique les manifests Kubernetes.

| √âtape | Capture |
| --- | --- |
| Tests unitaires | ![GitHub Actions - tests](docs/screenshots/github_action_tests.png) |
| Build & push | ![GitHub Actions - build](docs/screenshots/github_action_build.png) |
| D√©ploiement Minikube | ![GitHub Actions - deploy](docs/screenshots/github_action_deploy.png) |

Cr√©er un environnement GitHub Actions s√©curis√© :
- Le d√©p√¥t doit √™tre public pour que GHCR soit accessible sans credentials suppl√©mentaires (sinon, fournir un `imagePullSecret`).
- Ajuster les URLs du manifeste K8s si vous utilisez un autre endpoint MinIO/MLflow.

## Structure du d√©p√¥t

```
mlops-project/
‚îú‚îÄ‚îÄ airflow/dags/...
‚îú‚îÄ‚îÄ app/api/main.py          # API FastAPI
‚îú‚îÄ‚îÄ app/webapp/streamlit_app.py
‚îú‚îÄ‚îÄ models/                  # Mod√®le PyTorch & utilitaires
‚îú‚îÄ‚îÄ monitoring/              # Prometheus & Grafana config
‚îú‚îÄ‚îÄ k8s/minikube-manifest.yaml
‚îú‚îÄ‚îÄ scripts/bootstrap_minio.py
‚îú‚îÄ‚îÄ tests/test_utils.py
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ .github/workflows/ci_cd.yml
```

## Notes

- Dataset : 200 images par classe, r√©cup√©r√©es directement depuis GitHub.
- Les DAGs utilisent les variables Airflow (`data_base_path`, `image_size`, etc.) pour personnaliser les chemins.
- L'API t√©l√©charge automatiquement la derni√®re version du mod√®le depuis MinIO au d√©marrage.
- MLflow enregistre le mod√®le (`mlflow.pytorch.log_model`) et le meilleur checkpoint dans MinIO.
